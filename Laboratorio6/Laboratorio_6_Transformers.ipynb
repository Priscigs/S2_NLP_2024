{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCj1CZYDPVS"
      },
      "source": [
        "NOMBRE: Priscilla González Sandoval\n",
        "\n",
        "\n",
        "CARNE: 20689\n",
        "\n",
        "\n",
        "FECHA: 22.09.24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN22MYvxDrBp"
      },
      "source": [
        "Responda las siguientes preguntas:\n",
        "1. ¿Cuáles son los dos procesos principales de un transformer y qué función cumplen?\n",
        "  \n",
        "    *   A. Encoding:\n",
        "El input que se ingresa que en este caso puede ser cualquoer tipo de datp es transformado en una representación interna o codificación. La principal función del encoding es generar una representación contextualizada de cada elemento del input, teniendo en cuenta las relaciones entre las palabras o elementos dentro de la secuencia, utilizando mecanismos como la self-attention.\n",
        "\n",
        "    *   B. Decoding:\n",
        "Este toma la representación generada por el encoding y lo que hace es que la transforma en el output deseado. Mientras este proceso se realiza, el decoding también utiliza self-attention para generar cada token del output, prestando atención tanto al input como a la secuencia generada previamente.\n",
        "  \n",
        "2. Asigne cada uno de las siguientes subtareas a su proceso correspondiente(A o B): Multi-Head Self-Attention Mechanism,Position-wise Feed-Forward Networks,Masked Multi-Head Self-Attention Mechanism, Encoder-Decoder Multi-Head Attention,Position-wise Feed-Forward Networks\n",
        "\n",
        "    *   A. Codificación (Encoding):\n",
        "\n",
        "        *     Multi-Head Self-Attention Mechanism\n",
        "        *     Position-wise Feed-Forward Networks\n",
        "        \n",
        "    *   B. Decodificación (Decoding):\n",
        "\n",
        "        *     Masked Multi-Head Self-Attention Mechanism\n",
        "        *     Encoder-Decoder Multi-Head Attention\n",
        "        *     Position-wise Feed-Forward Networks\n",
        "\n",
        "Después de asignar, explique la función de cada una de las partes. Luego, proceda a programar un transformer con cada una de esas partes utilizando solo tensorflow o Pytorch.\n",
        "\n",
        "- Multi-Head Self-Attention Mechanism:\n",
        "\n",
        "    - Permite al modelo centrarse en diferentes posiciones de la secuencia del input simultáneamente. Cada \"head\" de atención obtiene una representación diferente de la información en cada token, lo que ayuda al modelo a entender mejor el contexto global. En la codificación, esto sirve para el análisis de las relaciones dentro de la secuencia de entrada.\n",
        "\n",
        "- Position-wise Feed-Forward Networks:\n",
        "\n",
        "    - Estas redes aplican una capa de feed-forward completamente conectada a cada token de manera independiente, tras las capas de atención. Actúan como transformaciones no lineales para mejorar la capacidad de representación de las capas de atención.\n",
        "\n",
        "- Masked Multi-Head Self-Attention Mechanism (Decodificación):\n",
        "\n",
        "    - Similar al Multi-Head Self-Attention Mechanism, solo que en este caso se utiliza una máscara para evitar que el modelo \"vea\" las posiciones futuras de la secuencia. De esta forma, el modelo genera un token a la vez.\n",
        "\n",
        "- Encoder-Decoder Multi-Head Attention (Decodificación):\n",
        "\n",
        "    - Esta capa permite que el decodificador preste atención a la secuencia generada por el codificador. Ayuda a que el modelo se enfoque en las partes más relevantes de la secuencia de entrada mientras genera la secuencia de salida.\n",
        "\n",
        "- Position-wise Feed-Forward Networks:\n",
        "\n",
        "    - Lo que hace es transformar cada vector de representación de manera individual y de manera idéntica en todas las posiciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3tVVmNYDqWA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0bYNL3FHBTM"
      },
      "source": [
        "Responda: ¿Qué es el posicional encoding y por qué debe definirse una función para los transformer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EShmMnX_DTMw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWzapD7QHYyr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qMe7xfaHY01"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F71jNfpEHY3P"
      },
      "outputs": [],
      "source": [
        "#Datos de prueba\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 8500\n",
        "target_vocab_size = 8000\n",
        "pe_input = 1000\n",
        "pe_target = 1000\n",
        "dropout_rate = 0.1\n",
        "\n",
        "#Su output shape debe ser: (64, 50, 8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-b0nbXyHa2P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
