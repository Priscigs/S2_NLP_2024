{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCj1CZYDPVS"
      },
      "source": [
        "NOMBRE: Priscilla González Sandoval\n",
        "\n",
        "\n",
        "CARNE: 20689\n",
        "\n",
        "\n",
        "FECHA: 22.09.24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN22MYvxDrBp"
      },
      "source": [
        "Responda las siguientes preguntas:\n",
        "1. ¿Cuáles son los dos procesos principales de un transformer y qué función cumplen?\n",
        "  \n",
        "    *   A. Encoding:\n",
        "El input que se ingresa que en este caso puede ser cualquoer tipo de datp es transformado en una representación interna o codificación. La principal función del encoding es generar una representación contextualizada de cada elemento del input, teniendo en cuenta las relaciones entre las palabras o elementos dentro de la secuencia, utilizando mecanismos como la self-attention.\n",
        "\n",
        "    *   B. Decoding:\n",
        "Este toma la representación generada por el encoding y lo que hace es que la transforma en el output deseado. Mientras este proceso se realiza, el decoding también utiliza self-attention para generar cada token del output, prestando atención tanto al input como a la secuencia generada previamente.\n",
        "  \n",
        "2. Asigne cada uno de las siguientes subtareas a su proceso correspondiente(A o B): Multi-Head Self-Attention Mechanism,Position-wise Feed-Forward Networks,Masked Multi-Head Self-Attention Mechanism, Encoder-Decoder Multi-Head Attention,Position-wise Feed-Forward Networks\n",
        "\n",
        "    *   A. Codificación (Encoding):\n",
        "\n",
        "        *     Multi-Head Self-Attention Mechanism\n",
        "        *     Position-wise Feed-Forward Networks\n",
        "        \n",
        "    *   B. Decodificación (Decoding):\n",
        "\n",
        "        *     Masked Multi-Head Self-Attention Mechanism\n",
        "        *     Encoder-Decoder Multi-Head Attention\n",
        "        *     Position-wise Feed-Forward Networks\n",
        "\n",
        "Después de asignar, explique la función de cada una de las partes. Luego, proceda a programar un transformer con cada una de esas partes utilizando solo tensorflow o Pytorch.\n",
        "\n",
        "- Multi-Head Self-Attention Mechanism:\n",
        "\n",
        "    - Permite al modelo centrarse en diferentes posiciones de la secuencia del input simultáneamente. Cada \"head\" de atención obtiene una representación diferente de la información en cada token, lo que ayuda al modelo a entender mejor el contexto global. En la codificación, esto sirve para el análisis de las relaciones dentro de la secuencia de entrada.\n",
        "\n",
        "- Position-wise Feed-Forward Networks:\n",
        "\n",
        "    - Estas redes aplican una capa de feed-forward completamente conectada a cada token de manera independiente, tras las capas de atención. Actúan como transformaciones no lineales para mejorar la capacidad de representación de las capas de atención.\n",
        "\n",
        "- Masked Multi-Head Self-Attention Mechanism (Decodificación):\n",
        "\n",
        "    - Similar al Multi-Head Self-Attention Mechanism, solo que en este caso se utiliza una máscara para evitar que el modelo \"vea\" las posiciones futuras de la secuencia. De esta forma, el modelo genera un token a la vez.\n",
        "\n",
        "- Encoder-Decoder Multi-Head Attention (Decodificación):\n",
        "\n",
        "    - Esta capa permite que el decodificador preste atención a la secuencia generada por el codificador. Ayuda a que el modelo se enfoque en las partes más relevantes de la secuencia de entrada mientras genera la secuencia de salida.\n",
        "\n",
        "- Position-wise Feed-Forward Networks:\n",
        "\n",
        "    - Lo que hace es transformar cada vector de representación de manera individual y de manera idéntica en todas las posiciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B3tVVmNYDqWA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0bYNL3FHBTM"
      },
      "source": [
        "Responda: ¿Qué es el posicional encoding y por qué debe definirse una función para los transformer?\n",
        "\n",
        "*   Este es importnnte porque hace que el modelo distinga entre diferentes posiciones en la secuencoa en la que se trabaja. Por lo tanto se debe definir ya que calcula un vector asignando un valor único a cada posición de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EShmMnX_DTMw"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IWzapD7QHYyr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        \n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        \n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        \n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0qMe7xfaHY01"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        \n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        \n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "        \n",
        "        return out3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, dropout_rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        \n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        \n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        \n",
        "        # Embedding + Positional Encoding\n",
        "        x = self.embedding(x)  \n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        \n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        # Pasar por las capas del codificador\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        \n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, dropout_rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        \n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        \n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        \n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        # Pasar por las capas del decodificador\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            \n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "        \n",
        "        return x, attention_weights  # (batch_size, target_seq_len, d_model), attention weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, dropout_rate)\n",
        "        \n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, dropout_rate)\n",
        "        \n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask,\n",
        "             look_ahead_mask, dec_padding_mask):\n",
        "        # Codificador\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # Decodificador\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)  # (batch_size, tar_seq_len, d_model)\n",
        "        \n",
        "        # Output final\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "# Máscaras (Simplificadas para este ejemplo)\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(seq_len):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F71jNfpEHY3P"
      },
      "outputs": [],
      "source": [
        "#Datos de prueba\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "input_vocab_size = 8500\n",
        "target_vocab_size = 8000\n",
        "pe_input = 1000\n",
        "pe_target = 1000\n",
        "dropout_rate = 0.1\n",
        "\n",
        "#Su output shape debe ser: (64, 50, 8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a-b0nbXyHa2P"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=pe_input,\n",
        "    pe_target=pe_target,\n",
        "    dropout_rate=dropout_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: False (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(64, 50), dtype=int32)\n  • tar=tf.Tensor(shape=(64, 50), dtype=int32)\n  • training=False\n  • enc_padding_mask=tf.Tensor(shape=(64, 1, 1, 50), dtype=float32)\n  • look_ahead_mask=tf.Tensor(shape=(1, 1, 50, 50), dtype=float32)\n  • dec_padding_mask=tf.Tensor(shape=(64, 1, 1, 50), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m final_output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdec_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_padding_mask\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape del Output Final:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_output\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "Cell \u001b[0;32mIn[15], line 223\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, tar, training, enc_padding_mask,\n\u001b[1;32m    222\u001b[0m          look_ahead_mask, dec_padding_mask):\n\u001b[0;32m--> 223\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     dec_output, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    226\u001b[0m         tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\u001b[1;32m    228\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(dec_output)\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: False (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(64, 50), dtype=int32)\n  • tar=tf.Tensor(shape=(64, 50), dtype=int32)\n  • training=False\n  • enc_padding_mask=tf.Tensor(shape=(64, 1, 1, 50), dtype=float32)\n  • look_ahead_mask=tf.Tensor(shape=(1, 1, 50, 50), dtype=float32)\n  • dec_padding_mask=tf.Tensor(shape=(64, 1, 1, 50), dtype=float32)"
          ]
        }
      ],
      "source": [
        "# Datos de prueba\n",
        "batch_size = 64\n",
        "inp_seq_len = 50  # Longitud de la secuencia de entrada\n",
        "tar_seq_len = 50  # Longitud de la secuencia de salida\n",
        "\n",
        "# Generar datos de entrada y salida aleatorios\n",
        "sample_input = tf.random.uniform((batch_size, inp_seq_len), minval=1, maxval=input_vocab_size, dtype=tf.int32)\n",
        "sample_target = tf.random.uniform((batch_size, tar_seq_len), minval=1, maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "# Crear máscaras\n",
        "enc_padding_mask = create_padding_mask(sample_input)\n",
        "dec_padding_mask = create_padding_mask(sample_target)  # Dec padding mask should be based on target\n",
        "look_ahead_mask = create_look_ahead_mask(tar_seq_len)\n",
        "\n",
        "# Forward Pass\n",
        "final_output, _ = transformer(\n",
        "    sample_input, \n",
        "    sample_target, \n",
        "    training=False, \n",
        "    enc_padding_mask=enc_padding_mask, \n",
        "    look_ahead_mask=look_ahead_mask, \n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "print(\"Shape del Output Final:\", final_output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
