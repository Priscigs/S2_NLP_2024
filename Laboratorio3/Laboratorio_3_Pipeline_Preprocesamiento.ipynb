{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvRApnRKSI9"
      },
      "source": [
        "NOMBRES: Fátima Priscilla\n",
        "\n",
        "APELLIDOS: González Sandoval\n",
        "\n",
        "CARNE: 20689\n",
        "\n",
        "FECHA: 25.07.24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-7-RdQDQsYE"
      },
      "source": [
        "**Instrucciones:**\n",
        "\n",
        "El objetivo de esta práctica es que apliquen lo visto en clase y en otros cursos. Al texto \"metamorphosis_kafka\" deben aplicarse técnicas de tokenización, lemmatización, stemming y normalización. Sin embargo, estos procesos no se aplican de manera indiscriminada. Para este proyecto tienen que aplicar las técnicas a **1 de 2 objetivos**.\n",
        "\n",
        "1. Este texto se usará para entrenar un LLM para que escriba con un estilo kafkiano.\n",
        "2. Reconocer textos escritor por Kafka. El objetivo final en esta línea sería poder etiquetar algo como \"kafka\" y \"no kafka\".\n",
        "\n",
        "Para ambas perspectivas qué tipos de tokenización, lemmatización, stemming y normalización utilizaría. Ej: tokenizaría por oraciones en lugares de palabras, tomaría o no en cuenta signos de puntuación, removería stopwords o no, etc. Si para esto consulta otras fuentes, citelas.\n",
        "\n",
        "Luego de explicar ambas perspectivas **escoja 1** y programe cada una de las etapas.\n",
        "\n",
        "Se recomienda que antes de empezar, revise el texto para ver si hay cosas que le conviene editar antes de empezar a programar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenización por oraciones: ['metamorphosis\\n\\nby franz kafka\\n\\ntranslated by david wyllie\\n\\n\\n\\n\\ni\\n\\n\\none morning, when gregor samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.', 'he lay on his\\narmour-like back, and if he lifted his head a little he could see his\\nbrown belly, slightly domed and divided by arches into stiff sections.', 'the bedding was hardly able to cover it and seemed ready to slide off\\nany moment.', 'his many legs, pitifully thin compared with the size of the\\nrest of him, waved about helplessly as he looked.', '“what’s happened to me?” he thought.']\n",
            "Lematización y normalización: ['metamorphosis', 'franz', 'kafka', 'translated', 'david']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Cargar el texto\n",
        "with open('metamorphosis_kafka.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Para leer solo el texto de kafka\n",
        "start_text = \"*** START OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\"\n",
        "end_text = \"*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\"\n",
        "\n",
        "start_idx = text.find(start_text) + len(start_text)\n",
        "end_idx = text.find(end_text)\n",
        "\n",
        "# Obtener solo el texto relevante\n",
        "text = text[start_idx:end_idx].strip()\n",
        "\n",
        "# Normalización\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenización\n",
        "words = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Lemmatización\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "filtered_words = [word for word in lemmatized_words if word not in stopwords.words('english') + list(string.punctuation)]\n",
        "\n",
        "print(\"Tokenización:\", sentences[:5])\n",
        "print(\"Lematización y normalización:\", filtered_words[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
