{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXhjY9dGwJls"
      },
      "source": [
        "NOMBRES: Fátima Priscilla \n",
        "\n",
        "APELLIDOS: González Sandoval\n",
        "\n",
        "CARNE: 20689\n",
        "\n",
        "FECHA: 24 de agosto de 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7nnvCPLh5Ms"
      },
      "source": [
        "**Ejercicio 1**\n",
        "Con los datos de la clase de lunes, cálcule PPMI, pero aplicando Lapace Smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3aTZXJeiFAI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf3GX1_liFXd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRf6sNDiF1v"
      },
      "source": [
        "**Ejercicio 2**\n",
        "\n",
        "POC para crear información de entreno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f16xWrpDiH-9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o3CzZjlIiPA2"
      },
      "outputs": [],
      "source": [
        "#Librerías que necesitarán\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "%load_ext tensorboard\n",
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e0LLReSPiVxw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['the', 'wind', 'crosses', 'the', 'brown', 'land', 'unheard']\n",
            "Vocabulario: {'algo importante para que no explote el programa': 0, 'the': 1, 'wind': 2, 'crosses': 3, 'brown': 4, 'land': 5, 'unheard': 6}\n",
            "Secuencia de ejemplo: [1, 2, 3, 1, 4, 5, 6]\n",
            "Positive Skip-Grams: [[1, 4], [3, 1], [3, 4], [1, 2], [5, 1], [1, 3], [5, 4], [2, 1], [4, 5], [6, 5], [4, 6], [2, 3], [4, 1], [2, 1], [6, 4], [1, 2], [1, 5], [3, 2], [5, 6], [3, 1], [4, 3], [1, 3]]\n"
          ]
        }
      ],
      "source": [
        "sentece = \"The wind Crosses the brown land unheard\" #De mis poemas fav. The waste land de T.S Elliot\n",
        "tokens = tf.keras.preprocessing.text.text_to_word_sequence(sentece) #tokenizar\n",
        "vocab, index = {}, 1 #arreglo vocabulario, empezar indice en 1\n",
        "vocab[\"algo importante para que no explote el programa\"] = 0\n",
        "\n",
        "#Llene el arreglo para vocabulario\n",
        "for word in tokens:\n",
        "    if word not in vocab:\n",
        "        vocab[word] = index\n",
        "        index += 1\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "example_sequence = [vocab[word] for word in tokens]\n",
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    example_sequence,\n",
        "    vocabulary_size=vocab_size,\n",
        "    window_size=window_size,\n",
        "    negative_samples=0\n",
        ") #rellene esta area como le sea pertinente)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Vocabulario:\", vocab)\n",
        "print(\"Secuencia de ejemplo:\", example_sequence)\n",
        "print(\"Positive Skip-Grams:\", positive_skip_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loA64ZR0k3L0",
        "outputId": "a8e28e58-e1bd-492c-c92b-13539c7a74c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 4): (the, brown)\n",
            "(3, 1): (crosses, the)\n",
            "(3, 4): (crosses, brown)\n",
            "(1, 2): (the, wind)\n",
            "(5, 1): (land, the)\n"
          ]
        }
      ],
      "source": [
        "#Resultados\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GdtE3R10lcAG"
      },
      "outputs": [],
      "source": [
        "target_word, context_word = positive_skip_grams[0]\n",
        "# Escoja un numero para muestras negativos (que no pertenence al contexto)\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # clase que debe ser muestreada como positiva (que pertenece al contexto)\n",
        "    num_true=1,\n",
        "    num_sampled=num_ns,\n",
        "    unique=True,\n",
        "    range_max=vocab_size,  # [0, vocab_size]\n",
        "    seed=SEED,\n",
        "    name=\"negative_sampling\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WcxIUEmpmUt7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
            "['wind', 'the', 'brown', 'crosses']\n"
          ]
        }
      ],
      "source": [
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OpVHwBkDAurO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
            " 0.01212381]\n"
          ]
        }
      ],
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=vocab_size) #funcion que construye la tabla de muestreo en forma de frecuencias\n",
        "print(sampling_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t3iK0LesA5Lu"
      },
      "outputs": [],
      "source": [
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1) #se agrega una dimension para poder concatenar\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label = tf.squeeze(label)\n",
        "\n",
        "#Listo! asi se prepara la info para entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNHYhsn6D-jS",
        "outputId": "6dee158e-2219-440c-f8bd-62d824c1489b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Informacion a utilizar:\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omu8ON00A_0H"
      },
      "outputs": [],
      "source": [
        "#Paso 1: cree una funcion que estandarice el texto como ya hemos hecho e incrustela en una capa de Tensorflow (TextVectorization)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y64v-qOFDhDj"
      },
      "outputs": [],
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h5OvM9GDrRt"
      },
      "outputs": [],
      "source": [
        "#Paso 2: Usando la POC de creacion de datos de entreno, defina una funcion para este proceso\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMmxeZPbEMOg"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsgvY5hZE5X4"
      },
      "outputs": [],
      "source": [
        "#El siguiente codigo es el modelo Word2Vec usando lo que ya han hecho. Sin embargo, deben agregar la metrica de similitud que vimos en clase, a la cual deben llamar dots\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                       embedding_dim)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "\n",
        "\n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCX5lVLdFN-D"
      },
      "outputs": [],
      "source": [
        "#Entrenar modelo y definir CategoricalCrossEntropy como funcion de perdida. Mostrar el print de accuracy y loss. Si deciden presentar sus resultados en tensorboard, adjuntar capturas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
