{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH_zPatjVVhE"
      },
      "source": [
        "NOMBRE: Priscilla González\n",
        "\n",
        "\n",
        "CARNE: 20689\n",
        "\n",
        "\n",
        "FECHA: 12.10.24\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBAfJsqmZhWg"
      },
      "source": [
        "**Objetivo**: La idea de esta práctica es que exploren el desarrollo de un proceso de cuatización. Para esto, deben cubrir lo que se vio en clase:\n",
        "1. Efectuar el proceso de cuantización.\n",
        "2. Determinar su error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "oWDLcURIVZqs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tensor = torch.tensor([[-0.6870,  0.2607, -0.7718,  0.0841],\n",
        "                       [ 0.6558, -1.1711, -1.0713,  0.2405],\n",
        "                       [ 1.4967, -0.6928,  0.7961, -1.4614],\n",
        "                       [ 0.5689, -0.5227, -1.3111, -0.8343]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Mt-KTIVrVt12"
      },
      "outputs": [],
      "source": [
        "rmin = test_tensor.min().item()\n",
        "rmax = test_tensor.max().item()\n",
        "qmin = -128\n",
        "qmax = 127\n",
        "\n",
        "scale = (rmax - rmin) / (qmax - qmin)\n",
        "zero_point = round(qmin - (rmin / scale))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rmin: -1.461400032043457, rmax: 1.4967000484466553\n",
            "Scale: 0.011600392472510244\n",
            "Zero point: -2\n"
          ]
        }
      ],
      "source": [
        "print(f\"rmin: {rmin}, rmax: {rmax}\")\n",
        "print(f\"Scale: {scale}\")\n",
        "print(f\"Zero point: {zero_point}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantized_tensor = torch.round(test_tensor / scale + zero_point).clamp(qmin, qmax).to(torch.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "deq_tensor = (quantized_tensor.float() - zero_point) * scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor cuantizado:\n",
            "tensor([[ -61,   20,  -69,    5],\n",
            "        [  55, -103,  -94,   19],\n",
            "        [ 127,  -62,   67, -128],\n",
            "        [  47,  -47, -115,  -74]], dtype=torch.int8)\n",
            "\n",
            "Tensor descuantizado:\n",
            "tensor([[-0.6844,  0.2552, -0.7772,  0.0812],\n",
            "        [ 0.6612, -1.1716, -1.0672,  0.2436],\n",
            "        [ 1.4965, -0.6960,  0.8004, -1.4616],\n",
            "        [ 0.5684, -0.5220, -1.3108, -0.8352]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Tensor cuantizado:\")\n",
        "print(quantized_tensor)\n",
        "print(\"\\nTensor descuantizado:\")\n",
        "print(deq_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Así como se vio en clase, definan una proceso de linear quantization para test_tensor\n",
        "#Para llevar esto a acabo, debe encontrar la escala: scale = rmax-rmin/qmax-qmin.\n",
        "#Trabajando con pytorch int8 (para mapear a int4) qmax=127 y qmin=-128.\n",
        "#zero_point = qmin-(rmin/scale).\n",
        "#Con esta informacion ya pueden proceder a trabaja la cuantización lineal.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Resultado del tensor cuantizado:\n",
        "quantized_tensor = tensor([[-77, -77, -77, -77],\n",
        "                           [-77, -77, -77, -77],\n",
        "                           [-77, -77, -77, -77],\n",
        "                           [-77, -77, -77, -77]], dtype=torch.int8)\n",
        "#Resultado del tensor descuantizado:\n",
        "deq_tensor = tensor([[0., 0., 0., 0.],\n",
        "                     [0., 0., 0., 0.],\n",
        "                     [0., 0., 0., 0.],\n",
        "                     [0., 0., 0., 0.]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y2z0ujTYdgf"
      },
      "source": [
        "Con la parte anterior correcta, ya pueden proceder a trabajar lo siguiente:\n",
        "\n",
        "1. Investigar Symmetric mode quantization.\n",
        "2. per-channel quantization.\n",
        "3. per-group quantization.\n",
        "\n",
        "Para cada uno de estos, explicar su funcionamiento y cuando puede ser conveniente usarlos respecto a las otras técnicas.\n",
        "\n",
        "Finalmente, desarrollar cada uno de la misma manera en que se trabajo la cuantización lineal. (Estos tres son casos especiales de la cuantización lineal, por lo que deberían solo hacer cambios especificos al código de la primera parte).\n",
        "\n",
        "\n",
        "Responda la siguiente pregunta: ¿Aquí estamos trabajando post-training quantization o quantization-aware training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Symmetric Mode Quantization\n",
        "\n",
        "Para este caso, los valores proceden a cuantizarse, de esta forma se encuentran valores tanto positivos como negativos con respecto a 0. Por lo tanto, los valores mínimos y máximos del tensor original se cuantizan de forma simétrica. Entonces, es bastante conveniente usarlo cuando los valores tienen una distribución centrada alrededor de 0, como se trata ReLU.\n",
        "\n",
        " - https://www.ai-bites.net/model-quantization-in-deep-learning/#:~:text=Symmetric%20quantization%20is%20exactly%20what,point%20is%20set%20to%200.\n",
        "\n",
        "### Per-channel Quantization\n",
        "\n",
        "Este es comúnmente usado en capas convolucionales ya que cada dimensión del tensor contiene su propia cuantización, en este caso, es el scale y el zero_point.\n",
        "\n",
        " - https://medium.com/@curiositydeck/quantization-granularity-aec2dd7a0bb4\n",
        "\n",
        "### Per-group Quantization\n",
        "\n",
        "Esta técnica es casi igual a la antrior, solo que en este caso, lo que se hace es que se agrupan varias dimensiones y no solamente 1. Entonces, de igual forma, se le aplica su propia cuantización por dimensión. Esta técnica ayuda bastante cuando se trata de aplicaciones con bastantes capas en redes neuronales.\n",
        "\n",
        "- https://medium.com/@curiositydeck/quantization-granularity-aec2dd7a0bb4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Symmetric Mode Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized Tensor:\n",
            " tensor([[-117,   44, -128,   14],\n",
            "        [ 112, -128, -128,   41],\n",
            "        [ 127, -118,  127, -128],\n",
            "        [  97,  -89, -128, -128]], dtype=torch.int8)\n",
            "Dequantized Tensor:\n",
            " tensor([[-0.6867,  0.2583, -0.7513,  0.0822],\n",
            "        [ 0.6574, -0.7513, -0.7513,  0.2406],\n",
            "        [ 0.7454, -0.6926,  0.7454, -0.7513],\n",
            "        [ 0.5693, -0.5224, -0.7513, -0.7513]])\n"
          ]
        }
      ],
      "source": [
        "scale_symmetric = max(abs(rmin), abs(rmax)) / (qmax - qmin)\n",
        "zero_point_symmetric = 0  \n",
        "\n",
        "quantized_tensor_symmetric = torch.round(test_tensor / scale_symmetric).clamp(qmin, qmax).to(torch.int8)\n",
        "\n",
        "deq_tensor_symmetric = quantized_tensor_symmetric.float() * scale_symmetric\n",
        "\n",
        "print(\"Quantized Tensor:\\n\", quantized_tensor_symmetric)\n",
        "print(\"Dequantized Tensor:\\n\", deq_tensor_symmetric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Per-channer Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized Tensor:\n",
            " tensor([[ -61,   20,  -69,    5],\n",
            "        [  55, -103,  -94,   19],\n",
            "        [ 127,  -62,   67, -128],\n",
            "        [  47,  -47, -115,  -74]], dtype=torch.int8)\n",
            "Desquantized Tensor:\n",
            " tensor([[-0.6844,  0.2552, -0.7772,  0.0812],\n",
            "        [ 0.6612, -1.1716, -1.0672,  0.2436],\n",
            "        [ 1.4965, -0.6960,  0.8004, -1.4616],\n",
            "        [ 0.5684, -0.5220, -1.3108, -0.8352]])\n"
          ]
        }
      ],
      "source": [
        "scales = []\n",
        "zero_points = []\n",
        "quantized_tensor_per_channel = torch.empty_like(test_tensor, dtype=torch.int8)\n",
        "\n",
        "rmin = test_tensor.min().item()\n",
        "rmax = test_tensor.max().item()\n",
        "scale = (rmax - rmin) / (qmax - qmin)\n",
        "zero_point = round(qmin - (rmin / scale))\n",
        "\n",
        "scales.append(scale)\n",
        "zero_points.append(zero_point)\n",
        "\n",
        "quantized_tensor_per_channel = torch.round(test_tensor / scale + zero_point).clamp(qmin, qmax).to(torch.int8)\n",
        "\n",
        "deq_tensor_per_channel = (quantized_tensor_per_channel.float() - zero_points[0]) * scales[0]\n",
        "\n",
        "print(\"Quantized Tensor:\\n\", quantized_tensor_per_channel)\n",
        "print(\"Desquantized Tensor:\\n\", deq_tensor_per_channel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Per-group Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized Tensor:\n",
            " tensor([[ -61,   71,  -73,   47],\n",
            "        [ 127, -128, -115,   69],\n",
            "        [ 127,  -62,   67, -128],\n",
            "        [  47,  -47, -115,  -74]], dtype=torch.int8)\n",
            "Desquantized Tensor:\n",
            " tensor([[-0.6878,  0.2579, -0.7737,  0.0860],\n",
            "        [ 0.6591, -1.1678, -1.0746,  0.2436],\n",
            "        [ 1.4965, -0.6960,  0.8004, -1.4616],\n",
            "        [ 0.5684, -0.5220, -1.3108, -0.8352]])\n"
          ]
        }
      ],
      "source": [
        "quantized_tensor_per_group = torch.empty_like(test_tensor, dtype=torch.int8) \n",
        "deq_tensor_per_group = torch.empty_like(test_tensor, dtype=torch.float32)  \n",
        "\n",
        "group_size = 2  \n",
        "\n",
        "for g in range(0, test_tensor.shape[0], group_size):  \n",
        "    rmin = test_tensor[g:g + group_size, :].min().item()\n",
        "    rmax = test_tensor[g:g + group_size, :].max().item()\n",
        "    \n",
        "    scale = (rmax - rmin) / (qmax - qmin)\n",
        "    zero_point = round(qmin - (rmin / scale))\n",
        "    \n",
        "    quantized_tensor_per_group[g:g + group_size, :] = torch.round(test_tensor[g:g + group_size, :] / scale + zero_point).clamp(qmin, qmax).to(torch.int8)\n",
        "    \n",
        "    deq_tensor_per_group[g:g + group_size, :] = (quantized_tensor_per_group[g:g + group_size, :].float() - zero_point) * scale\n",
        "\n",
        "print(\"Quantized Tensor:\\n\", quantized_tensor_per_group)\n",
        "print(\"Desquantized Tensor:\\n\", deq_tensor_per_group)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se está utilizando post-training potque en este caso el tensor ya está especificado y luego se le aplica la cuantización ya cuando este ya está practicamente entrenado"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
